{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando técnicas de tratamiento de textos\n",
    "\n",
    "En este notebook aplicaremos algunas de las téncicas que hemos visto en clase. Además, incluiremos alguna que otra herramienta nueva que nos vendrá muy bien para extraer el máximo de nuestros textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nONQt2yzChS"
   },
   "source": [
    "### Importamos las librerías necesarias y descarga las stopwords de nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "Mw6CalghzChT",
    "outputId": "668fb506-16b5-4495-d64b-49a56ff8f3b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TheBridge\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9aiit_zChW"
   },
   "source": [
    "## Corpus de muestra\n",
    "\n",
    "El corpus es el vocabulario que utilizaremos en nuestro análisis de textos. Este vocabulario puede ser obtenido de alguna de las múltiples fuentes que nos encontraremos por Internet (que podrán variar según la temática estudiada, y aparecerán en diferentes idiomas), y algunas librerías de análisis de texto ya traen incorporadas. Otra opción sería crearnos uno propio. Finalmente, la opción más rápida para muestras reducidas consiste en extraerlo de la propia fuente de datos que se va a analizar, como haremos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "gwma4atgzChX",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8e39d572-1ae4-4e40-fb97-373b9efca6d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category\n",
       "0                     The sky is blue and beautiful.  weather\n",
       "1                  Love this blue and beautiful sky!  weather\n",
       "2       The quick brown fox jumps over the lazy dog.  animals\n",
       "3   The brown fox is quick and the blue dog is lazy!  animals\n",
       "4  The sky is very blue and the sky is very beaut...  weather\n",
       "5        The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El corpus estará formado por un texto y una clasificación, que ayudarán a nuestro algoritmo a identificar patrones. \n",
    "# Pero para que pueda trabajar, tendremos que tratar el corpus antes:\n",
    "\n",
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'animals', 'weather', 'animals']\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVSzcV5YzCha"
   },
   "source": [
    "## Preprocesamiento básico de texto\n",
    "\n",
    "Tras conseguir el corpus, tendremos que procesar los textos. Si nos fijamos en el lenguaje, hay muchas formas de expresar lo mismo, y hay muchas palabras que, pese a que sean importantes gramaticalmente, no aportan información relevante, como son las stopwords. Por ejemplo: ``the`` o ``is``.\n",
    "\n",
    "Estas palabras podemos identificarlas en diferentes frases, por lo que no aportan información que nos hagan identificar una como algo especial, es decir, no aportan significado.\n",
    "\n",
    "Pongamos un ejemplo con la primera frase: \"the sky is blue and beautiful\":\n",
    "\n",
    "Si observamos esta frase, que está marcada como \"weather\", identificamos algunas palabras que tienen significado por sí mismas, como son \"sky\", \"blue\", incluso \"beautiful\". Sin embargo, palabras como \"the\" o \"is\" aparecen en la mayoría de frases y no tienen un significado por sí mismas que nos ayuden a identificarlas con algo. Precisamente por esta condición, estas palabras están en la lista de stopwords. Por lo tanto, estas palabras las deberemos quitar para no confundir al modelo, ya que no aportan nada y solo harán que sea más fácil que interprete mal los datos y emplee más tiempo de computación.\n",
    "\n",
    "Por ello, nos vamos a crear una función que normalice los textos que le pasemos, entendiendo _normalizar_ por quedarse con aquellas palabras que aporten valor. Veamos poco a poco esta función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "bsG37EXQzChb"
   },
   "outputs": [],
   "source": [
    "# Comenzamos importando un elemento de nltk para tokenizar:\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "# Y las stopwords, que quitaremos de nuestros textos:\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc, test=True):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    # Eliminamos todo aquello que no sea un caracter alfanumérico o un espacio:\n",
    "    # ^ = distinto d3\n",
    "    # \\s = espacio\n",
    "    # a-z = de la a a la z (minúsculas). A-Z = mayúsculas. 0-9 = del 0 al 9\n",
    "    # '' = sustituir por '' (= eliminar)\n",
    "    # re.I = ignora si son mayúsculas o minúsculas\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
    "    \n",
    "    # Convertimos a minúsculas:\n",
    "    doc = doc.lower()\n",
    "    # Quitamos espacios que sobran:\n",
    "    doc = doc.strip()\n",
    "    # Tokenizamos el texto:\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # Si queremos que se impriman, ponemos ese parámetro a True:\n",
    "    if test:\n",
    "        print(tokens)\n",
    "    # Una vez tokenizado, tendremos una lista de palabras. Eliminamos las que estén en las stopwords:\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Si queremos que se impriman, ponemos ese parámetro a True:\n",
    "    if test:\n",
    "        print(filtered_tokens)\n",
    "    # Finalmente, recreamos un único string con todas las palabras separadas por espacios\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya tenemos nuestra función que normaliza frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sky', 'is', 'blue', 'and', 'beautiful']\n",
      "['sky', 'blue', 'beautiful']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sky blue beautiful'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_document(\"the sky is blue and beautiful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, lo que queremos es utilizarlo con unas cuántas entradas, es decir, como si estuviéramos trabajando con un array de textos. Para ello, podemos utilizar la función ``np.vectorize()``. Veamos cómo funciona con un ejemplo:\n",
    "\n",
    "Vamos a crearnos una función que recibe un parámetro y devuelve otro parámetro, en concreto, vamos a utilizar una función que nos diga si un número es par o no:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_par(m):\n",
    "    if m%2 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_par(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_par(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, funciona con números sueltos pero, ¿qué pasa si usamos una lista o un array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-ed4c03ef098f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mes_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-167-321de01e862e>\u001b[0m in \u001b[0;36mes_par\u001b[1;34m(m)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mes_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "es_par(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función recibe un conjunto de datos en lugar de un número solo, por lo que no sabe qué hacer y falla. En cambio, si usamos la función que hemos comentado antes, ``np.vectorize()`` esto se soluciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False,  True, False])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_par_vector = np.vectorize(es_par)\n",
    "# Ahora ya es posible usarla con arrays\n",
    "m = np.array([1, 2, 3, 4, 5])\n",
    "es_par_vector(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes comprobar, la función vectorizada nos permite utilizar conjuntos de datos, así que si lo usamos sobre nuestra función de normalización de frases, podremos utilizarla con un array de textos, como es la variable ``corpus``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The sky is blue and beautiful.',\n",
       "       'Love this blue and beautiful sky!',\n",
       "       'The quick brown fox jumps over the lazy dog.',\n",
       "       'The brown fox is quick and the blue dog is lazy!',\n",
       "       'The sky is very blue and the sky is very beautiful today',\n",
       "       'The dog is lazy but the brown fox is quick!'], dtype='<U56')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sky', 'is', 'blue', 'and', 'beautiful']\n",
      "['sky', 'blue', 'beautiful']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sky blue beautiful'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = 'The sky is blue and beautiful.'\n",
    "normalize_document(frase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'sky', 'is', 'blue', 'and', 'beautiful']\n",
      "['sky', 'blue', 'beautiful']\n",
      "['the', 'sky', 'is', 'blue', 'and', 'beautiful']\n",
      "['sky', 'blue', 'beautiful']\n",
      "['love', 'this', 'blue', 'and', 'beautiful', 'sky']\n",
      "['love', 'blue', 'beautiful', 'sky']\n",
      "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
      "['the', 'brown', 'fox', 'is', 'quick', 'and', 'the', 'blue', 'dog', 'is', 'lazy']\n",
      "['brown', 'fox', 'quick', 'blue', 'dog', 'lazy']\n",
      "['the', 'sky', 'is', 'very', 'blue', 'and', 'the', 'sky', 'is', 'very', 'beautiful', 'today']\n",
      "['sky', 'blue', 'sky', 'beautiful', 'today']\n",
      "['the', 'dog', 'is', 'lazy', 'but', 'the', 'brown', 'fox', 'is', 'quick']\n",
      "['dog', 'lazy', 'brown', 'fox', 'quick']\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "h3Gs4WimybgG",
    "outputId": "08f89898-c535-4a09-bdd6-5fb66693518f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog', 'brown fox quick blue dog lazy',\n",
       "       'sky blue sky beautiful today', 'dog lazy brown fox quick'],\n",
       "      dtype='<U30')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Prueba a utilizar la función de normalización que hemos usado anteriormente con una lista de frases en inglés a ver qué obtienes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Hemos visto que también existen paquetes de stopwords en español. ¿Podrías replicar la función que acabamos de ver para tokenizar nuestras frases pero adaptándola al castellano?\n",
    "\n",
    "Pruébala con unas frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yVLO7myzChg"
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "Una vez tenemos nuestro corpus normalizado, podemos pasar a utilizar diferentes técnicas. En este caso, comenzaremos por la ya comentaada en clase Bag of Words, para la cual utilizábamos el objeto CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "collapsed": false,
    "id": "tSa-cqPjzChh",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "84b1ff09-232e-4c8c-b9d8-dd9570084883"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1],\n",
       "       [0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos ha devuelto un array, pero podemos recuperar las palabras a partir del método ``get_feature_names()``. Podríamos, incluso, hacer las comprobaciones de que ha hecho lo que le hemos pedido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "zP6VHVkTzChk",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "a5e8def5-8ab1-4056-e484-342637b396d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sky</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beautiful  blue  brown  dog  fox  jumps  lazy  love  quick  sky  today\n",
       "0          1     1      0    0    0      0     0     0      0    1      0\n",
       "1          1     1      0    0    0      0     0     1      0    1      0\n",
       "2          0     0      1    1    1      1     1     0      1    0      0\n",
       "3          0     1      1    1    1      0     1     0      1    0      0\n",
       "4          1     1      0    0    0      0     0     0      0    2      1\n",
       "5          0     0      1    1    1      0     1     0      1    0      0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = cv.get_feature_names()\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDKfW8IUzChm"
   },
   "source": [
    "# Bag of N-Grams\n",
    "\n",
    "Como hemos visto, Bag of words es una particularización de Bag of n-grams para n=1. Sin embargo, si en lugar de palabras sueltas queremos añadir contexto a nuestro análisis, podemos elegir n-grams de nivel superior. Por ejemplo, de 2 elementos (bigramas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "collapsed": false,
    "id": "4jty5YpHzChn",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4cbd06ad-cb88-4fe8-e987-72c7ef9c35f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 17)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bv = CountVectorizer(ngram_range=(2,2))\n",
    "bv_matrix = bv.fit_transform(norm_corpus)\n",
    "bv_matrix = bv_matrix.toarray()\n",
    "vocab = bv.get_feature_names()\n",
    "bigramas= pd.DataFrame(bv_matrix, columns=vocab)\n",
    "bigramas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "MdzUEjJHR33n",
    "outputId": "34afa233-b4a6-41b4-a64e-a1a5ccd6962f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful sky</th>\n",
       "      <th>beautiful today</th>\n",
       "      <th>blue beautiful</th>\n",
       "      <th>blue dog</th>\n",
       "      <th>blue sky</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog lazy</th>\n",
       "      <th>fox jumps</th>\n",
       "      <th>fox quick</th>\n",
       "      <th>jumps lazy</th>\n",
       "      <th>lazy brown</th>\n",
       "      <th>lazy dog</th>\n",
       "      <th>love blue</th>\n",
       "      <th>quick blue</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>sky beautiful</th>\n",
       "      <th>sky blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beautiful sky  beautiful today  blue beautiful  blue dog  blue sky  \\\n",
       "0              0                0               1         0         0   \n",
       "1              1                0               1         0         0   \n",
       "2              0                0               0         0         0   \n",
       "3              0                0               0         1         0   \n",
       "4              0                1               0         0         1   \n",
       "5              0                0               0         0         0   \n",
       "\n",
       "   brown fox  dog lazy  fox jumps  fox quick  jumps lazy  lazy brown  \\\n",
       "0          0         0          0          0           0           0   \n",
       "1          0         0          0          0           0           0   \n",
       "2          1         0          1          0           1           0   \n",
       "3          1         1          0          1           0           0   \n",
       "4          0         0          0          0           0           0   \n",
       "5          1         1          0          1           0           1   \n",
       "\n",
       "   lazy dog  love blue  quick blue  quick brown  sky beautiful  sky blue  \n",
       "0         0          0           0            0              0         1  \n",
       "1         0          1           0            0              0         0  \n",
       "2         1          0           0            1              0         0  \n",
       "3         0          0           1            0              0         0  \n",
       "4         0          0           0            0              1         1  \n",
       "5         0          0           0            0              0         0  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFAKSu8hzChp"
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "Por último, antes de pasar a la siguiente fase, vamos a ver una tercera opción. Se trata de la transformación TF-IDF, cuyas siglas vienen del inglés _Term-Frequency times Inverse Document-Frequency_ (en castellano sería algo así como “frecuencia del término por frecuencia inversa de documento”).\n",
    "\n",
    "Básicamente, se trata de un método que se basa en una ponderación de los términos en función de su relevancia. Para ello, calcula el número de veces que aparece en un documento, y lo divide entre un factor que aumenta con el número de apariciones en otros documentos. De este modo, cuando un término aparece muchas veces en muchos documentos, no será relevante pues no será algo que permita identificar a ese documento en concreto. Sin embargo, si un término aparece muchas veces en un documento, pero pocas en el resto, será un factor particular de esos pocos documentos que lo contienen, por lo que será una posible característica común que los relacione entre sí.\n",
    "\n",
    "La fórmula por la que se rige es la siguiente:\n",
    "\n",
    "\n",
    "$$\n",
    "tfidf(t, d) = tf(t,d) * idf(t)\n",
    "$$\n",
    "\n",
    "donde $t$ is un término (o palabra, si estamos con n-gramas de n=1) y $d$ es un documento (es decir, uno de los textos de nuestro dataset)\n",
    "\n",
    "Además, $tf(t, d)$ hace referencia al número de términos en el documento actual, mientras que $idf(t)$ viene definida de la siguiente manera:\n",
    "\n",
    "$$\n",
    "idf(t) = log(\\frac{1 + n_d}{1 + df(d, t)}) +1\n",
    "$$\n",
    "\n",
    "donde $n_d$ es el número de documentos y $df(d,t)$ es el número de documentos que contienen el término $t$\n",
    "\n",
    "Si quieres conocer en profundidad esta técnica, puedes consultar la [documentación de scikit-learn](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)\n",
    "\n",
    "\n",
    "Veamos cómo implementarlo de manera sencilla con el objeto correspondiente, cuya documentación podrás encontrar [aquí](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "qp74wH6AzChq",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "50899291-e870-4752-cf3b-3e7c631ae9c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>fox</th>\n",
       "      <th>jumps</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sky</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beautiful  blue  brown   dog   fox  jumps  lazy  love  quick   sky  today\n",
       "0       0.60  0.52   0.00  0.00  0.00   0.00  0.00  0.00   0.00  0.60   0.00\n",
       "1       0.46  0.39   0.00  0.00  0.00   0.00  0.00  0.66   0.00  0.46   0.00\n",
       "2       0.00  0.00   0.38  0.38  0.38   0.54  0.38  0.00   0.38  0.00   0.00\n",
       "3       0.00  0.36   0.42  0.42  0.42   0.00  0.42  0.00   0.42  0.00   0.00\n",
       "4       0.36  0.31   0.00  0.00  0.00   0.00  0.00  0.00   0.00  0.72   0.52\n",
       "5       0.00  0.00   0.45  0.45  0.45   0.00  0.45  0.00   0.45  0.00   0.00"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(use_idf=True)\n",
    "tv_matrix = tv.fit_transform(norm_corpus)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "df_matrix = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
    "df_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "La función TF-IDF se engloba dentro del IR (Information Retrieval, Recuperación de Información), que se encarga de rankear documentos dentro de una gran selección de los mismos, como hacen los buscadores de Internet. Google, por ejemplo, utiliza este tipo de técnicas para saber qué devolverte, y en qué orden, cuando realizas una búsqueda. Evidentemente, los motores de Google utilizan algoritmos mucho más complejos, pero parten de esta base.\n",
    "\n",
    "Imagina que creas un buscador que se basa en la TF-IDF:\n",
    "1. ¿Qué documento devolverías si buscan el término _beautiful_?\n",
    "2. ¿Y si buscan el término _sky_?\n",
    "3. Crea una función que reciba una matriz de estas características (con un número de columnas que no sepamos de antemano) y devuelva el docuento (o los documentos, si hay empate) que devolvería nuestro buscador en primer lugar.\n",
    "\n",
    "La función recibirá un dataframe con esa matriz y un término, que deberá buscar en ella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ¿Podrías replicar en una función todo el proceso, desde la lista de textos hasta que devuelva el texto (o los textos) que mejor se identifique(n) con el término buscado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7JlMRxkzChs"
   },
   "source": [
    "## Similitud de documentos\n",
    "\n",
    "Una vez hemos elegido una de las 3 técnicas anteriores, podemos pasar a calcular la matriz de similitud, que nos permite ver cuánto de parecidas son dos textos entre sí. Lo necesitaremos para continuar con nuestro estudio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "Dz8oW5d0zChs",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "71ad6d10-b5f7-4c19-cf7c-84e0db804fcb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.753128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.807539</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.753128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.608181</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.784362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.185447</td>\n",
       "      <td>0.139665</td>\n",
       "      <td>0.784362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109653</td>\n",
       "      <td>0.933779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.807539</td>\n",
       "      <td>0.608181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109653</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839987</td>\n",
       "      <td>0.933779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  1.000000  0.753128  0.000000  0.185447  0.807539  0.000000\n",
       "1  0.753128  1.000000  0.000000  0.139665  0.608181  0.000000\n",
       "2  0.000000  0.000000  1.000000  0.784362  0.000000  0.839987\n",
       "3  0.185447  0.139665  0.784362  1.000000  0.109653  0.933779\n",
       "4  0.807539  0.608181  0.000000  0.109653  1.000000  0.000000\n",
       "5  0.000000  0.000000  0.839987  0.933779  0.000000  1.000000"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta gráfica podemos interpretar que el texto 3 es el más próximo al texto 5, lo que significa que son muy parecidos. Mientras que es el más alejado al texto 4, lo que significa que no se parecen. Lo podemos comprobar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown fox quick blue dog lazy\n",
      "dog lazy brown fox quick\n"
     ]
    }
   ],
   "source": [
    "print(normalize_document(corpus[3], test=False))\n",
    "print(normalize_document(corpus[5], test=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown fox quick blue dog lazy\n",
      "sky blue sky beautiful today\n"
     ]
    }
   ],
   "source": [
    "print(normalize_document(corpus[3], test=False))\n",
    "print(normalize_document(corpus[4], test=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Prueba a realizar la similitud de documentos usando el modelo Bag of n-grams, con diferentes valores de n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv_vH9brzChu"
   },
   "source": [
    "### Clustering de documentos usando la matriz de similitud\n",
    "\n",
    "Con la matriz de similitud podemos identificar clusters de textos. En este caso, utilizaremos el algoritmo KMeans, que es un algoritmo de clustering que nos permite estudiar los patrones que relacionan los registros entre sí para crear grupos de características similares. Como veremos cuando lleguemos a ello, este algoritmo necesita que le digamos cuántos grupos queremos que busque, que en este caso le diremos 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "x7Xd7eMqzChv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6d1d6f3a-2953-4b07-85c4-fd3dea3bba0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category  ClusterLabel\n",
       "0                     The sky is blue and beautiful.  weather             0\n",
       "1                  Love this blue and beautiful sky!  weather             0\n",
       "2       The quick brown fox jumps over the lazy dog.  animals             1\n",
       "3   The brown fox is quick and the blue dog is lazy!  animals             1\n",
       "4  The sky is very blue and the sky is very beaut...  weather             0\n",
       "5        The dog is lazy but the brown fox is quick!  animals             1"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "km.fit_transform(similarity_df)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, parece que la categoría se ha identificado bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Como hemos visto, hemos podido realizar un modelo desde el comienzo hasta el final. Repasa lo que hemos visto hasta aquí e intenta, utilizando los métodos ``transform`` de los objetos que ya han sido entrenados con las muestras, realizar la predicción de una frase nueva. Créate una función que te devuelva la predicción de una frase. ¿Podrías realizarlo para que recibiera una lista de textos? (En aquellos puntos que existan más de una alternativa, elije la que te apetezca, o añade un parámetro para que se pueda decidir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yweeXV9ZzChx"
   },
   "source": [
    "### Modelo de tópicos\n",
    "\n",
    "Otra opción que se nos presenta como alternativa a la matriz de similitud es el modelo de tópicos.\n",
    "\n",
    "Los tópicos hacen referencia a la temática del texto. Con el modelo de tópicos, lo que buscamos es extraer temáticas comunes entre los textos. En este caso, vamos a utilizar un método similar al visto con PCA, LDA. Ambos se tratan de métodos de reducción de dimensionalidad. Ene ste caso, usaremos LDA porque los elementos que nos devuelve suman 1, por lo que son complementarioas y nos permitirá realizar una comparación porcentual entre los tópicos.\n",
    "\n",
    "En este caso, y como hemos estado haciendo anteriormente, le tendremos que indicar cuántos tópicos queremos buscar. En este caso, para que tenga cierta coherencia con lo que hemos visto anteriormente, indicaremos 2 grupos (aunque con más también funcionaría)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "rEvcsvuvzChx",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "1c15b3f1-ea4b-4d34-a567-cf4f71ffc18b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190548</td>\n",
       "      <td>0.809452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.176804</td>\n",
       "      <td>0.823196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.846184</td>\n",
       "      <td>0.153816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814863</td>\n",
       "      <td>0.185137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180516</td>\n",
       "      <td>0.819484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.839172</td>\n",
       "      <td>0.160828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         T1        T2\n",
       "0  0.190548  0.809452\n",
       "1  0.176804  0.823196\n",
       "2  0.846184  0.153816\n",
       "3  0.814863  0.185137\n",
       "4  0.180516  0.819484\n",
       "5  0.839172  0.160828"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=2, max_iter=100, random_state=42)\n",
    "dt_matrix = lda.fit_transform(tv_matrix)\n",
    "features = pd.DataFrame(dt_matrix, columns=['T1', 'T2'])\n",
    "features\n",
    "\n",
    "# Los dos tópicos suman 1\n",
    "# El tópico mayoritario te da la clase única\n",
    "# Lo interesante es ver la proporción de cada tópico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fScUGR8YzChz"
   },
   "source": [
    "### Mostrando los tópicos y sus pesos\n",
    "\n",
    "Dado que hemos utilizado el modelo LDA, y tenemos el vocabulario que hemos utilziado, podemos asociarlo a cada tópico y qué peso se le ha adjudicado. A mayor peso, mayor importancia tendrá dentro de ese grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "collapsed": false,
    "id": "9DCs6JTpzCh0",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "4088257b-325d-41c3-9cdb-b9f323031466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('brown', 1.7273638692668465), ('dog', 1.7273638692668465), ('fox', 1.7273638692668465), ('lazy', 1.7273638692668465), ('quick', 1.7273638692668465), ('jumps', 1.0328325272484777), ('blue', 0.7731573162915626)]\n",
      "\n",
      "[('sky', 2.264386643135622), ('beautiful', 1.9068269319456903), ('blue', 1.7996282104933266), ('love', 1.148127242397004), ('today', 1.0068251160429935)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt_matrix = lda.components_\n",
    "for topic_weights in tt_matrix:\n",
    "    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n",
    "    topic = sorted(topic, key=lambda x: -x[1]) # ordeno por el peso decreciente\n",
    "    topic = [item for item in topic if item[1] > 0.6] # me quedo solo con los pesos mayores de 0.6\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up2RJ99dzCh2"
   },
   "source": [
    "## Clustering de documentos usando los tópicos\n",
    "\n",
    "Al igual que hemos visto para la matriz de similitud, también podemos realizar el clustering de documentos a base del modelo de tópicos que acabamos de ver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "W6w9TOaBzCh2",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "dc3d5887-b24f-419d-fa26-41ceeb4c64ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category  ClusterLabel\n",
       "0                     The sky is blue and beautiful.  weather             1\n",
       "1                  Love this blue and beautiful sky!  weather             1\n",
       "2       The quick brown fox jumps over the lazy dog.  animals             0\n",
       "3   The brown fox is quick and the blue dog is lazy!  animals             0\n",
       "4  The sky is very blue and the sky is very beaut...  weather             1\n",
       "5        The dog is lazy but the brown fox is quick!  animals             0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=2)\n",
    "km.fit_transform(features)\n",
    "cluster_labels = km.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)\n",
    "\n",
    "# ahora agrupo en clusters usando los tópicos, no la similaridad basada en cosenos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EJERCICIO\n",
    "\n",
    "Como hemos visto, hemos podido realizar un modelo desde el comienzo hasta el final. Repasa lo que hemos visto hasta aquí e intenta, utilizando los métodos ``transform`` de los objetos que ya han sido entrenados con las muestras, realizar la predicción de una frase nueva. Créate una función que te devuelva la predicción de una frase. ¿Podrías realizarlo para que recibiera una lista de textos? (Sí, es el mismo ejercicio que en el apartado anterior, solo que en este deberás hacerlo basándote en los tópicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instalamos gensim. PAra ello, abre el Anaconda Prompt e introduce el siguiente comando:\n",
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7RRX2LezCh5"
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Principalmente, la técnica de word embedding se basa en el mapeo de las palabras en vectores. Para representar un ejemplo de cómo funcinoan, utilizaremos el modelo word2vec, que por debajo utilizará una red neuronal para entender relaciones entre palabras (así como sinónimos) y convertir los textos en vectores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "id": "GMTrPk6HzCh6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "# Tokenizamos el corpus (parecido a lo que hemos hecho anteriormente):\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n",
    "\n",
    "# Establecemos algunos parámetros (que dependerá de lo que queramos):\n",
    "feature_size = 10    # dimensión del vector de palabra  \n",
    "window_context = 10          # tamaño de la ventana de contexto                                                                                    \n",
    "min_word_count = 1   # mínimo conteo de palabras                        \n",
    "sample = 1e-3   # Downsample para palabras frecuentes\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
    "                          window=window_context, min_count = min_word_count,\n",
    "                          sample=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdNSrx2BzSWM"
   },
   "source": [
    "size: el número de dimensiones de los embeddings, cuyo valor predeterminado es 100. Es decir, el número de dimensiones\n",
    "\n",
    "window: la distancia máxima entre una palabra de destino y palabras alrededor de la palabra de destino. La ventana predeterminada es 5.\n",
    "\n",
    "min_count: el recuento mínimo de palabras a considerar al entrenar el modelo; las palabras con menos ocurrencia que este recuento serán ignoradas. El valor predeterminado para min_count es 5.\n",
    "\n",
    "Una vez creado el modelo, podemos utilizarlo para convertir las palabras que conoce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "collapsed": false,
    "id": "hIaT477zzCh8",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6d865e05-6537-4058-8cff-373424d6d938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01799249,  0.00436799, -0.02310739, -0.00020116,  0.00576331,\n",
       "       -0.04385744,  0.0302899 ,  0.03762824,  0.00314637,  0.01500313],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['sky']\n",
    "# de palabra a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cambio, si no las conoce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "collapsed": false,
    "id": "hIaT477zzCh8",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6d865e05-6537-4058-8cff-373424d6d938"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'ball' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-caeea90e326a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ball'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# de palabra a vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'ball' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w2v_model.wv['ball']\n",
    "# de palabra a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar algún método de clustering sobre el word2vec, debemos calcular la media de cada uno de los vectores, que nos permitirá conocer el centro de cada palabra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "HTg0vKMTzCh-"
   },
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model.wv, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "collapsed": false,
    "id": "JY4qgba1zCiA",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b7956d0c-20aa-4b08-ab50-1ebb74774314"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016909</td>\n",
       "      <td>-0.003709</td>\n",
       "      <td>-0.004981</td>\n",
       "      <td>0.016603</td>\n",
       "      <td>-0.015143</td>\n",
       "      <td>-0.019532</td>\n",
       "      <td>0.016493</td>\n",
       "      <td>0.025477</td>\n",
       "      <td>-0.005502</td>\n",
       "      <td>-0.013718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009661</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>-0.014291</td>\n",
       "      <td>0.011890</td>\n",
       "      <td>-0.010202</td>\n",
       "      <td>-0.010888</td>\n",
       "      <td>0.016157</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>-0.014890</td>\n",
       "      <td>-0.011488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.023557</td>\n",
       "      <td>-0.003475</td>\n",
       "      <td>-0.012394</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>0.007172</td>\n",
       "      <td>0.001734</td>\n",
       "      <td>-0.007579</td>\n",
       "      <td>-0.002404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008641</td>\n",
       "      <td>0.016370</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>-0.010445</td>\n",
       "      <td>-0.001654</td>\n",
       "      <td>0.015274</td>\n",
       "      <td>0.009776</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>-0.002274</td>\n",
       "      <td>-0.015978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012068</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>-0.013391</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>-0.023976</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>-0.004638</td>\n",
       "      <td>-0.001197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005978</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-0.000962</td>\n",
       "      <td>0.020895</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>-0.003255</td>\n",
       "      <td>-0.012595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.016909 -0.003709 -0.004981  0.016603 -0.015143 -0.019532  0.016493   \n",
       "1  0.009661  0.003391 -0.014291  0.011890 -0.010202 -0.010888  0.016157   \n",
       "2  0.003789  0.023557 -0.003475 -0.012394 -0.003350  0.010498  0.007172   \n",
       "3  0.008641  0.016370  0.001927 -0.010445 -0.001654  0.015274  0.009776   \n",
       "4  0.012068  0.004309 -0.013391  0.002914 -0.001207 -0.023976  0.008053   \n",
       "5  0.005978  0.022547 -0.004632 -0.012665 -0.000962  0.020895  0.006168   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.025477 -0.005502 -0.013718  \n",
       "1  0.027039 -0.014890 -0.011488  \n",
       "2  0.001734 -0.007579 -0.002404  \n",
       "3  0.005908 -0.002274 -0.015978  \n",
       "4  0.024023 -0.004638 -0.001197  \n",
       "5 -0.000075 -0.003255 -0.012595  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model, num_features=feature_size)\n",
    "pd.DataFrame(w2v_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "collapsed": false,
    "id": "xbSslODazCiC",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "912273b4-c993-4dfc-8f47-529363acc951"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheBridge\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:146: FutureWarning: 'random_state' has been introduced in 0.23. It will be set to None starting from 0.25 which means that results will differ at every function call. Set 'random_state' to None to silence this warning, or to 0 to keep the behavior of versions <0.23.\n",
      "  warnings.warn((\"'random_state' has been introduced in 0.23. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "      <th>ClusterLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category  ClusterLabel\n",
       "0                     The sky is blue and beautiful.  weather             0\n",
       "1                  Love this blue and beautiful sky!  weather             0\n",
       "2       The quick brown fox jumps over the lazy dog.  animals             1\n",
       "3   The brown fox is quick and the blue dog is lazy!  animals             1\n",
       "4  The sky is very blue and the sky is very beaut...  weather             0\n",
       "5        The dog is lazy but the brown fox is quick!  animals             1"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "# en affinitypropagation no se especifica el número de clusters, lo encuentra él\n",
    "\n",
    "ap = AffinityPropagation()\n",
    "ap.fit(w2v_feature_array)\n",
    "cluster_labels = ap.labels_\n",
    "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
    "pd.concat([corpus_df, cluster_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras cosas de word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-215-476b830517a8>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar('king')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7037114500999451),\n",
       " ('emperor', 0.7028552293777466),\n",
       " ('queen', 0.7002654075622559),\n",
       " ('kings', 0.6861088275909424),\n",
       " ('throne', 0.6725918054580688),\n",
       " ('vii', 0.6368046998977661),\n",
       " ('constantine', 0.6340476870536804),\n",
       " ('viii', 0.6330025792121887),\n",
       " ('pope', 0.6292762160301208),\n",
       " ('regent', 0.6254761219024658)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parecidos:\n",
    "model.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-219-f25ffeedec45>:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(positive = ['king', 'woman'], negative=['man'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6587183475494385),\n",
       " ('empress', 0.6027647256851196),\n",
       " ('prince', 0.6007430553436279),\n",
       " ('princess', 0.5861362814903259),\n",
       " ('son', 0.5810158252716064),\n",
       " ('daughter', 0.5689166188240051),\n",
       " ('elizabeth', 0.5688531398773193),\n",
       " ('throne', 0.5586293339729309),\n",
       " ('matilda', 0.5540385246276855),\n",
       " ('emperor', 0.5529552698135376)]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combinaciones:\n",
    "model.most_similar(positive = ['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-230-1ca03a2672fa>:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(positive = ['house', 'christian'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('church', 0.6582942008972168),\n",
       " ('congregation', 0.6448785066604614),\n",
       " ('priesthood', 0.6291549205780029),\n",
       " ('lds', 0.6249029636383057),\n",
       " ('lords', 0.6206523180007935),\n",
       " ('fathers', 0.6029797792434692),\n",
       " ('judaism', 0.60124671459198),\n",
       " ('calvinist', 0.6001708507537842),\n",
       " ('reformed', 0.599743127822876),\n",
       " ('protestant', 0.5970990657806396)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combinaciones:\n",
    "model.most_similar(positive = ['house', 'christian'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "COLAB_PROFE_2_Ejercicios_TextData.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
